{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyCBC Inference 3: Results files and plotting\n",
    "### Collin Capano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we take a closer look at the output files, including how to load samples from them. We then illustrate more advanced options in `pycbc_inference_plot_posterior`, and show how to create a posterior file. We will use an already complete results file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pycbc lalsuite ligo-common --no-cache-dir\n",
    "\n",
    "# This is needed to access the executables on sciserver. On a personal machine this should be ignore.\n",
    "path = %env PATH\n",
    "%env PATH=$path:/home/idies/miniconda3/envs/py27/bin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download a result file from a fully completed analysis, from [here](https://www.atlas.aei.uni-hannover.de/~cdcapano/projects/pycbc_inference/workshop-may2019/bbh_injection). In this analysis, we injected a binary black hole simulation into LIGO data (20 seconds after GW150914). The injection parameters are:\n",
    "```\n",
    "tc = 1126259482.420\n",
    "mass1 = 37\n",
    "mass2 = 32\n",
    "ra = 2.2\n",
    "dec = -1.25\n",
    "inclincation = 2.5\n",
    "coa_phase = 1.5\n",
    "polarization = 1.75\n",
    "distance = 100\n",
    "f_ref = 20\n",
    "f_lower = 18\n",
    "approximant = IMRPhenomPv2\n",
    "taper = start\n",
    "```\n",
    "These are similar to GW150914, although it is about a factor of 5 closer in distance. You can see the config file used and the run script in the [linked directory](https://www.atlas.aei.uni-hannover.de/~cdcapano/projects/pycbc_inference/workshop-may2019/bbh_injection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('bbh_results.hdf'):\n",
    "    !wget https://www.atlas.aei.uni-hannover.de/~cdcapano/projects/pycbc_inference/bbh_injection/bbh_results.hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results file `bbh_results.hdf` is an HDF file. We could use the standard python module for reading HDF files [h5py](http://docs.h5py.org/en/stable/) to examine it. However, there are classes in [pycbc.inference.io](http://pycbc.org/pycbc/latest/html/pycbc.inference.io.html) that inherit from [h5py.File](http://docs.h5py.org/en/stable/high/file.html#file-objects) and add several convenience functions that make it easier to read samples from the file. There is one class for each type of sampler. So to load the file, we will use [pycbc.inference.io.loadfile](http://pycbc.org/pycbc/latest/html/pycbc.inference.io.html#pycbc.inference.io.loadfile). This function automatically determines which class to use to read the file, based on what is in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc.inference.io import loadfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = loadfile('bbh_results.hdf', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-level groups in the HDF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF files also have `attrs`. These are basically dictionaries that can be used to store metadata. Let's look at what metadata `pycbc_inference` stored in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.attrs.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler info\n",
    "The `sampler_info` group contains information about the sampler. The data that's stored in this group is sampler-specific. This is what was stored for this run, which used `emcee_pt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp['sampler_info'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every group can have their own `.attrs`; let's look at the `sampler_info` group's `attrs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp['sampler_info'].attrs.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injection data\n",
    "If we are analyzing an injection, as in this case, an `injections` group is added to the file. This group contains all the information about the injection(s) that was (were) performed. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp['injections'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp['injections'].attrs.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there was no data sets stored in the `injections` group (`.keys()` returned an empty list). All of the injection info was in the `.attrs`. This was because a single injection was performed. If multiple injections had been done, the parameters that were varied would be stored as datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples\n",
    "\n",
    "The samples that the sampler produced are stored in the `samples` group. There is one data set for each variable parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fp['samples'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional data may be stored in the `samples` group `attrs`. In this case, because we used the `marinalized_phase` model, the log likelihood of the noise $\\log p(d|n) = -\\left<d,\\,d\\right>/2$ is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp['samples'].attrs.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the shape of the datasets in the samples group. They all have the same shape, so we can just look at one of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fp['samples/mass1'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the dataset is `ntemps x nwalkers x n thinned iteration`. This run used 4 temperatures and 1000 walkers. Due to `max-samples-per-chain` being set to 1000, the data set has been thinned to include 750 samples from each walker and temperature.\n",
    "\n",
    "If this had been an `emcee` run (which uses no temperatures), the samples datasets would have been two dimensional: `nwalkers x niterations`. If it had been a nested sampling run (with either CPNest or Multinest), the datasets would have been 1 dimensional. **The format of the samples data is sampler dependent.** This is why we have separate classes to read the results file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load samples\n",
    "\n",
    "Now lets load the samples. We can use the `read_samples` function to do this. This takes as a the first argument a list of parameters to load. Here, we'll load all of the parameters. It also takes additional keyword arguments. These arguments are sampler-specific. For `emcee_pt` if we provide no additional keyword arguments, we'll get all temperatures. We just want the coldest temperature `temp=0`, as that is the posterior.\n",
    "\n",
    "If we provide no other arguments, `read_samples` will load all of the independent samples post burn-in. That is, it will get samples from all walkers, starting from the burn in iteration, and thinned by the ACL (it gets this information from the file's `.attrs`; specifically, the `thin_start` and `thin_interval` attributes). The samples are flattened into a 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = fp.read_samples(fp['samples'].keys(), temps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 7000 independent samples. What sort of object is `samples`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples have been returned as a [FieldArray](http://pycbc.org/pycbc/latest/html/pycbc.io.html#pycbc.io.record.FieldArray). A `FieldArray` is a wrapper around [`numpy` structured arrays](https://docs.scipy.org/doc/numpy/user/basics.rec.html) (actually, it inherits from `numpy`'s [record array](https://docs.scipy.org/doc/numpy/user/basics.rec.html#record-arrays), which in turn inherits from structured arrays). Structured arrays allow multiple parameters to be stored in a single array. `FieldArray` adds several convenience features, which we will look at in a bit. Since they inherit from structured arrays, a `FieldArray` can be indexed by parameter name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['mass1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you do, you get back a normal numpy array of the values stored for that parameter. We can see the full list of parameters that are stored using `.fieldnames` (This is an attribute added by `FieldArray`; it does not work for general structured arrays.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.fieldnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot `mass1` and `mass2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a scatter plot of the two mass parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass1 = samples['mass1']\n",
    "mass2 = samples['mass2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "ax.scatter(mass1, mass2)\n",
    "ax.set_xlabel('mass1')\n",
    "ax.set_ylabel('mass2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it converged, but something is odd here... we normally define `mass1 >= mass2`. But it looks like in our posterior samples, some points have `mass2 > mass1`. Lets confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mass1 >= mass2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The masses in the output file do not respect the convention that `mass1 >= mass2`.** This is expected: our prior (which was uniform for both masses in range $[10, 80)\\,\\mathrm{M}_\\odot$) was symmetric between the masses.\n",
    "\n",
    "But for plotting purposes, we want `mass1 >= mass2`. To do that, we can take advantage of one `FieldArray`'s key features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [pycbc.conversions](http://pycbc.org/pycbc/latest/html/pycbc.html#module-pycbc.conversions) provides a number of useful functions for converting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc import conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included in the conversions module, the [primary_mass](http://pycbc.org/pycbc/latest/html/pycbc.html#pycbc.conversions.primary_mass) function pulls out the larger of the two masses in array of masses. Likewise, the [secondary_mass](http://pycbc.org/pycbc/latest/html/pycbc.html#pycbc.conversions.secondary_mass) function pulls out the smaller of the two. This can do what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass1 = conversions.primary_mass(samples['mass1'], samples['mass2'])\n",
    "mass2 = conversions.secondary_mass(samples['mass1'], samples['mass2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mass1 >= mass2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "ax.scatter(mass1, mass2)\n",
    "ax.set_xlabel('mass1')\n",
    "ax.set_ylabel('mass2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using functions with Field Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not need to import and call the `primary_mass` and `secondary_mass` functions to apply them to the samples. Alternatively, we could of just passed these functions to the `samples` `FieldArray` as if they were parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass1 = samples['primary_mass(mass1, mass2)']\n",
    "mass2 = samples['secondary_mass(mass1, mass2)']\n",
    "(mass1 >= mass2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This highlights the main purpose of `FieldArray`:\n",
    "\n",
    " * **`FieldArrays` can be passed functions of their parameters as strings.**\n",
    "\n",
    "The available functions are any function in the [pycbc.conversions](http://pycbc.org/pycbc/latest/html/pycbc.html#module-pycbc.conversions), [pycbc.coordinates](http://pycbc.org/pycbc/latest/html/pycbc.html#module-pycbc.coordinates), or [pycbc.cosmology](http://pycbc.org/pycbc/latest/html/pycbc.html#module-pycbc.cosmology) modules. In addition, numpy math functions are available for use. The syntax is standard python.\n",
    "\n",
    "For example, if we wanted `spin1z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['spin1_a * cos(spin1_polar)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or redshift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['redshift(distance)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `--parameters` option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of `FieldArrays` is that it means functions of parameters can be passed on the command line to plotting programs using the `--parameters` option. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pycbc_inference_plot_posterior \\\n",
    "    --input-file bbh_results.hdf \\\n",
    "    --output-file mass1_mass2.png \\\n",
    "    --parameters 'primary_mass(mass1, mass2):mass1' 'secondary_mass(mass1, mass2):mass2' \\\n",
    "    --plot-scatter \\\n",
    "    --plot-contours \\\n",
    "    --plot-marginal \\\n",
    "    --z-arg snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='mass1_mass2.png', width=640, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, any program with a `--parameters` option can take functions of parameters. Note that this option also allows you to pass a label for the parameters. The syntax for this is `--parameters parameter_to_plot:label`. If a label is provided that is recognized parameter in the [pycbc.waveform.waveform.parameters](https://github.com/gwastro/pycbc/blob/master/pycbc/waveform/parameters.py) module, the predefined label will be used for that parameter in the plot. This is why when we passed `mass1` and `mass2` as the labels, we got $m_1$ $m_2$ in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File help\n",
    "Like all executables in `pycbc`, `pycbc_inference_plot_posterior` has a help message for its available options. The help can be seen by running the program with `--help` (or `-h`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pycbc_inference_plot_posterior --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be difficult to remember all of the functions and parameters that you have available to you. For this reason, if you give the option `--file-help` (or `-H`), along with an input file, you will get a file-specific help message that tells you what parameters are available for plotting in the file, along with all of the functions you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pycbc_inference_plot_posterior --input-file bbh_results.hdf --file-help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this also reveals additional options that you can provide to the program. These options are sampler-specific (and thus file-specific), which is why they do not appear in the general help message. For example, passing a `--temps` option works for a file that was created by `emcee_pt`, but not for a file that was created by `emcee`. If you're ever unsure what is available, just run the `-H` option with your input file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "Make a posterior plot of chirp mass (\"mchirp\") vs eta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating posterior files with `extract_samples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can get very tedious typing long functions on the command line every time you want to create a plot. Also, you may want to release your posterior samples along with a paper. Having different file formats depending on the sampler would make such a release confusing. Plus, the full samples file contains much more information than is necessary if you just want the posterior samples.\n",
    "\n",
    "The program `pycbc_inference_extract_samples` can convert a samples file into a *posterior file*. Regardless of the sampler that was used:\n",
    "\n",
    " * **Posterior files all have a \"samples\" group that contains 1D datasets of the posterior samples.**\n",
    " \n",
    "In process of creating the file, `pycbc_inference_extract_samples` can also be used to write out functions of the parameters as new datasets. This is because it also takes a `--parameters` option, except that here the labels part gives the name of the parameter in the new file. Nuisance parameters can be excluded, along with the other groups, such as `sampler_info`.\n",
    "\n",
    "For example, in the following, we create a posterior file containing the component masses (which we sort such that `mass1` $\\geq$ `mass2`) and the log likelihood. We also add the chirp mass and symmetric mass ratio `eta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pycbc_inference_extract_samples --verbose \\\n",
    "    --input-file bbh_results.hdf \\\n",
    "    --output-file mass_posterior.hdf \\\n",
    "    --parameters \\\n",
    "        'primary_mass(mass1, mass2):mass1' \\\n",
    "        'secondary_mass(mass1, mass2):mass2' \\\n",
    "        'mchirp_from_mass1_mass2(mass1, mass2):mchirp' \\\n",
    "        'eta_from_mass1_mass2(mass1, mass2):eta' \\\n",
    "        loglikelihood \\\n",
    "    --skip-groups data sampler_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the posterior file and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = loadfile('mass_posterior.hdf', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp['samples'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp['samples']['mchirp'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.attrs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = fp.read_samples(['mass1', 'mass2'])\n",
    "mass1 = posterior_samples['mass1']\n",
    "mass2 = posterior_samples['mass2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(mass1 >= mass2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the posterior file is much smaller than the full samples file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh bbh_results.hdf mass_posterior.hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a posterior plot of all of the mass parameters using the posterior file. We no longer need to pass the functions on the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pycbc_inference_plot_posterior \\\n",
    "    --input-file mass_posterior.hdf \\\n",
    "    --output-file posterior_mass1_mass2.png \\\n",
    "    --parameters mass1 mass2 mchirp eta \\\n",
    "    --plot-scatter \\\n",
    "    --plot-contours \\\n",
    "    --plot-marginal \\\n",
    "    --z-arg snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='posterior_mass1_mass2.png', width=640, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "Use the `--expected-parameters` option to put red lines at the injected values in the above plot. Read the output of `--help` to see the syntax that you need to provide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
